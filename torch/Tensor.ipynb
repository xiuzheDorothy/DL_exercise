{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Tenosr\n",
    "## 基本Tensor操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.8227e-30,  1.9250e-37,  1.0768e-35],\n",
      "        [-1.1538e-29, -8.7251e-04,  9.3731e-39]])\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]\n"
     ]
    }
   ],
   "source": [
    "a=t.Tensor(2,3)   # 指定形状Tensor\n",
    "b=t.Tensor([[1,2,3],[4,5,6]])   # list→Tensor\n",
    "c=b.tolist()    #Tensor→list\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "torch.Size([2, 3])\n",
      "6\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "b_size=b.size()\n",
    "print(b_size)   #Tensor.size()返回Torch.size()对象\n",
    "print(b.shape)  #tensor.shape可以直接达到和tensor.size()相同的效果，但它不是方法，不用加括号\n",
    "print(b.numel())\n",
    "print(b.nelement())   #numel() 和nelement()作用相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0768e-35, -3.4871e-06, -1.7712e+28],\n",
      "        [ 9.3731e-39,  5.2180e-38,  1.9249e-37]])\n",
      "tensor([2., 3.])\n"
     ]
    }
   ],
   "source": [
    "c=t.Tensor(b_size)   #既然b_size为 Torch.size()对象，则可以用做确定Tensor大小的参数\n",
    "d=t.Tensor((2,3))    #注意和a=t.Tensor(2,3)区别\n",
    "print(c)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 其他创建Tensor的方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([1, 3, 5, 7])\n",
      "tensor([ 1.0000,  5.5000, 10.0000])\n",
      "tensor([[ 0.6726,  0.9863,  0.1588],\n",
      "        [ 1.1156,  0.6533, -0.1911]])\n",
      "tensor([[0.3090, 0.9330, 0.2625],\n",
      "        [0.7111, 0.0111, 0.1755]])\n",
      "tensor([4, 0, 2, 1, 3])\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(t.ones(2,3))\n",
    "print(t.zeros(2,3))\n",
    "print(t.arange(1,8,2))   #从1到8，每次步长为2\n",
    "print(t.linspace(1,10,3))#1到10，分为3部分\n",
    "print(t.randn(2,3))  #标准正态分布\n",
    "print(t.rand(2,3))   #均匀分布\n",
    "print(t.randperm(5))  #长度为5的随机排列\n",
    "print(t.eye(2,3))  #对角线为1，不要求行列一致"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 常用Tensor操作\n",
    "通过tensor.view()方法可以调整tensor的形状，比如将1行6列的数据调整为2行三列，但view操作不会改变计算机存储数据的方式，只是输出时的读取方式不同，view之后的新tensor与原tensor共享统一内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n"
     ]
    }
   ],
   "source": [
    "a=t.arange(0,6)\n",
    "print(a)\n",
    "a=a.view(2,3)\n",
    "print(a)\n",
    "b=a.view(-1,3)  #-1表示按另一维度自动计算大小\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unsqueeze()和squeeze()用于改变数据的维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0, 1, 2],\n",
      "         [3, 4, 5]]])\n",
      "\n",
      " tensor([[[0, 1, 2]],\n",
      "\n",
      "        [[3, 4, 5]]])\n",
      "\n",
      " tensor([[[0],\n",
      "         [1],\n",
      "         [2]],\n",
      "\n",
      "        [[3],\n",
      "         [4],\n",
      "         [5]]])\n"
     ]
    }
   ],
   "source": [
    "print(b.unsqueeze(0))    #维度为1*2*3\n",
    "print('\\n',b.unsqueeze(1))    #维度为2*1*3\n",
    "print('\\n',b.unsqueeze(2))    #维度为2*3*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[[0, 1, 2],\n",
      "           [3, 4, 5]]]]])\n"
     ]
    }
   ],
   "source": [
    "c=b.view(1,1,1,2,3)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0, 1, 2],\n",
      "          [3, 4, 5]]]])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n"
     ]
    }
   ],
   "source": [
    "print(c.squeeze(0))  #压缩0维\n",
    "d=c\n",
    "for i in range(100):  #维度大于1的就无法压缩了\n",
    "    d=d.squeeze(0)\n",
    "print(d)\n",
    "print(c.squeeze()) #将所有维度为1的压缩"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "resize()是另一种用来调整size的方法,但它可以修改tensor的尺寸(不同于view)，即可以自动分配内存空间\n",
    "**从存储的角度讲，对tensor的操作可以分为两类：**\n",
    "- 不会修改自身数据，如a.add(b)，加法的结果返回一个新的tensor\n",
    "- 会修改自身数据，a.add_(b)，加法的结果仍存储在a中\n",
    "因为resize是会修改自身数据的，所以它的形式为：b.resize_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "tensor([[0, 1, 2]])\n",
      "tensor([[0, 1, 2]])\n"
     ]
    }
   ],
   "source": [
    "print(b)\n",
    "print(b.resize_(1,3))\n",
    "print(b) #此时b已经改变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[                  0,                   1,                   2],\n",
      "        [                  3,                   4,                   5],\n",
      "        [8033049682695424637, 7023439528312271989, 2308669004906786925]])\n"
     ]
    }
   ],
   "source": [
    "print(b.resize_(3,3))  #如果没有其他操作覆盖这一块区域，原来的数据是会保留的，但多出来的数据会分配存储空间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 索引操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4325, -0.9110,  1.6723,  0.4172],\n",
      "        [-1.1078, -0.7501, -0.4187, -1.6028],\n",
      "        [-0.9100,  1.1734, -1.8369, -1.1034]])\n",
      "torch.Size([3, 4])\n",
      "tensor([-0.4325, -0.9110,  1.6723,  0.4172])\n",
      "tensor([-0.4325, -0.9110,  1.6723,  0.4172])\n",
      "tensor([-0.4325, -1.1078, -0.9100])\n"
     ]
    }
   ],
   "source": [
    "a=t.randn(3,4)\n",
    "print(a)\n",
    "print(a.shape)\n",
    "print(a[0])  #第一个维度(数为3)选取0，第二个维度(数为4)选取全部\n",
    "print(a[0,:])#同上\n",
    "print(a[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4325, -0.9110,  1.6723,  0.4172],\n",
      "        [-1.1078, -0.7501, -0.4187, -1.6028]])\n",
      "tensor([[-0.4325, -0.9110],\n",
      "        [-1.1078, -0.7501]])\n"
     ]
    }
   ],
   "source": [
    "print(a[:2])  #前两行\n",
    "print(a[:2,0:2]) #前两行，前两列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False,  True, False],\n",
       "        [False, False, False, False],\n",
       "        [False,  True, False, False]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a > 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.6723, 1.1734])\n",
      "tensor([1.6723, 1.1734])\n"
     ]
    }
   ],
   "source": [
    "b=a[a>1] #挑选出所有大于1的，等价于a.masked_select(a>1)\n",
    "print(b)\n",
    "print(a.masked_select(a>1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4325, -0.9110,  1.6723,  0.4172],\n",
       "        [-1.1078, -0.7501, -0.4187, -1.6028]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[t.LongTensor([0,1])] #第0行和第1行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**其他常用选择函数**\n",
    " \n",
    " |函数|功能|\n",
    " |-----|----|\n",
    " |index_select(input,dim,index)|在指定dim上选取某些行和列|\n",
    " |masked_select(input,mask)|同a[a>0]，使用ByteTensor选取|\n",
    " |non_zero(input)|非零元素的下标|\n",
    " |gather(input,dim,index)|根据index，在dim维度上选取数据，输出的size与index一样|\n",
    "    \n",
    "**gather()的具体示例如下：**\n",
    "1. 取对角线元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2, 3]]) \n",
      "\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11],\n",
      "        [12, 13, 14, 15]]) \n",
      "\n",
      "tensor([[ 0,  5, 10, 15]])\n",
      "tensor([[0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [3]]) \n",
      "\n",
      "tensor([[ 0],\n",
      "        [ 5],\n",
      "        [10],\n",
      "        [15]])\n"
     ]
    }
   ],
   "source": [
    "index=t.LongTensor([[0,1,2,3]])\n",
    "print(index,'\\n') #第一个维度的数为1\n",
    "a=t.arange(0,16).view(4,4)\n",
    "print(a,'\\n')\n",
    "print(a.gather(0,index))\n",
    "'''\n",
    "0表示对第一个维度操作，然后按index的顺序依次取\n",
    "即按行操作：第一行，第二行，第三行。。。，每一行按照index的顺序取\n",
    "'''\n",
    "index=t.LongTensor([[0,1,2,3]]).t()\n",
    "print(index,'\\n') #第二个维度的数为1 ，即4*1\n",
    "print(a.gather(1,index)) # 在第二个维度选取数据，依次取0号，1号，2,号。。。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 取反对角线元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[12,  9,  6,  3]])\n",
      "tensor([[ 3],\n",
      "        [ 6],\n",
      "        [ 9],\n",
      "        [12]])\n"
     ]
    }
   ],
   "source": [
    "index=t.LongTensor([[3,2,1,0]])\n",
    "# print(index,'\\n') #第二个维度的数为1 ，即4*1\n",
    "print(a.gather(0,index))\n",
    "index=index.t()\n",
    "print(a.gather(1,index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 取两个对角线上元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  5, 10, 15],\n",
      "        [12,  9,  6,  3]])\n",
      "tensor([[ 0,  3],\n",
      "        [ 5,  6],\n",
      "        [10,  9],\n",
      "        [15, 12]])\n"
     ]
    }
   ],
   "source": [
    "index=t.LongTensor([[0,1,2,3],[3,2,1,0]])\n",
    "print(a.gather(0,index))\n",
    "index=index.t()\n",
    "b=a.gather(1,index)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与gather相应的逆操作是scatter_，sactter_把取出来的数据再放回去，<front color=red>注意scatter_是inplace操作</front>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  0.,  0.,  3.],\n",
       "        [ 0.,  5.,  6.,  0.],\n",
       "        [ 0.,  9., 10.,  0.],\n",
       "        [12.,  0.,  0., 15.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c=t.zeros(4,4)\n",
    "c.scatter_(1,index,b.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面这行代码如果按照原书中的写法会报错，报错如下：\n",
    "\n",
    "\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\n",
    "\n",
    "RuntimeError&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;Traceback (most recent call last)\n",
    "\n",
    "< ipython-input-26-8c806181a3e0 > in < module >\n",
    "\n",
    "&emsp;&emsp;&emsp;1 c=t.zeros(4,4)\n",
    "\n",
    "\\-\\-\\-\\-\\-> 2 c.scatter_(1,index,b)\n",
    "\n",
    "RuntimeError: Expected object of scalar type Float but got scalar type Long for argument #4 'src' in call to _th_scatter_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 高级索引\n",
    "略\n",
    "## Tensor类型\n",
    "默认的tensor为FloatTensor，可以通过t.set_default_tensor_type修改默认类型\n",
    "各种类型之间可以相互转换，type(new_type)是通用的做法\n",
    "CPU tensor和GPU tensor之间的互相转换通过tensor.cuda和tensor.cpu实现\n",
    "同时Tensor还有一个new方法，用法与t.Tensor()一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]], dtype=torch.float32)\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]], dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "t.set_default_tensor_type('torch.DoubleTensor')\n",
    "a=t.Tensor(2,3)\n",
    "print(a)\n",
    "b=a.float()\n",
    "print(b)\n",
    "c=a.type_as(b)\n",
    "print(c)\n",
    "t.set_default_tensor_type('torch.FloatTensor')  #还原为默认"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随着版本更新，原书代码会报错：\n",
    "TypeError: only floating-point types are supported as the default type\n",
    "因为Int型现在不支持设置为default，只能设置float类型的值为默认类型(float，double和half)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 逐元素操作\n",
    "这部分操作会对tensor的每一个元素进行操作\n",
    "\n",
    "![](https://s2.ax1x.com/2020/01/29/1QAD0A.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]]) \n",
      "\n",
      "tensor([[3, 3, 3],\n",
      "        [3, 4, 5]])\n"
     ]
    }
   ],
   "source": [
    "a=t.arange(0,6).view(2,3)\n",
    "print(a,'\\n')\n",
    "print(t.clamp(a,min=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 归并操作\n",
    "![](https://s2.ax1x.com/2020/01/29/1QEJBj.png)\n",
    "以上函数大多都有参数dim，用来指定在哪一个维度上进行操作\n",
    "假设输入的形状为(m,n,k):\n",
    "- dim=0，输出形状(1,n,k)或(n,k)\n",
    "- dim=1，输出形状(m,1,k)或(m,k)\n",
    "- dim=2，输出形状(m,n,1)或(m,n)\n",
    "\n",
    "size中是否具有1，取决于参数keepdim，keepdim=True就会保留维度1（pytorch 0.2.0 起keepdim默认为False）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2.]])\n",
      "tensor([2., 2., 2.])\n",
      "tensor([3., 3.])\n"
     ]
    }
   ],
   "source": [
    "b=t.ones(2,3)\n",
    "print(b.sum(dim=0,keepdim=True))\n",
    "print(b.sum(0)) #注意区别\n",
    "print(b.sum(dim=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
