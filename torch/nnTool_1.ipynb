{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络工具箱Torch.nn第一节：构造模型的方法\n",
    "torch.nn其是专门为深度学习而设计的模块。torch.nn的核心数据结构是`Module`，它是一个抽象概念，既可以表示神经网络中的某个层（layer），也可以表示一个**包含很多层的神经网络**。在实际使用中，最常见的做法是继承`nn.Module`，撰写自己的网络/层。\n",
    "\n",
    "----\n",
    "**本节主要参考资料为：**apacheCN的[pytorch文档](https://pytorch.apachecn.org/docs/1.2/)、ShusenTang改写的pytorch版[《动手学深度学习》](http://tangshusen.me/Dive-into-DL-PyTorch/#/ '《动手学深度学习》')和陈云的[《深度学习框架PYTORCH入门与实践》](https://github.com/chenyuntc/pytorch-book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from torch import nn #import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**一个神经网络的典型训练过程如下：**\n",
    "\n",
    "- 定义包含一些可学习参数（或者叫权重）的神经网络\n",
    "- 在输入数据集上迭代\n",
    "- 通过网络处理输入\n",
    "- 计算损失（输出和正确答案的距离）\n",
    "- 将梯度反向传播给网络的参数\n",
    "- 更新网络的权重，一般使用一个简单的规则：`weight = weight - learning_rate * gradient`\n",
    "\n",
    "## 继承`Module`类来构造模型\n",
    "下面继承`Module`类构造多层感知机，定义的`MLP`类重载了`Module`类的`__init__`函数和`forward`函数，分别用于创建模型参数和定义前向计算(正向传播)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(MLP,self).__init__(**kwargs)\n",
    "        # super()调用MLP父类Module的构造函数__init__进行初始化，\n",
    "        # 这样在构造实例时还可以指定其他函数的的参数\n",
    "        self.hidden = nn.Linear(784,256)\n",
    "        self.act = nn.ReLU() #激活函数\n",
    "        self.output =nn.Linear(256,10) # 输出层\n",
    "        \n",
    "    def forward(self,x):\n",
    "        a=self.act(self.hidden(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上的`MLP`类中无须定义反向传播函数。系统将通过**自动求梯度**而自动生成反向传播所需的`backward`函数。\n",
    "\n",
    "----\n",
    ">\\*args和\\**kwargs主要用于函数定义。你可以将不定数量的参数传递给某个函数\n",
    "\n",
    ">\\*args是用来发送一个非键值对的可变数量的参数列表给一个函数.\n",
    "\n",
    ">\\**kwargs允许你将不定长度的键值对作为参数传递给一个函数。如果你想要在一个函数里处理带名字的参数，你应该使用\\**kwargs\n",
    "\n",
    "----\n",
    "我们可以实例化`MLP`类得到**模型变量**`net`。下面的代码初始化`net`并传入输入数据X做一次前向计算。<font color=red>其中，net(X)会调用MLP继承自Module类的``__call__``函数，这个函数将调用MLP类定义的`forward`函数来完成前向计算。</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (hidden): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (act): ReLU()\n",
      "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net=MLP()\n",
    "X=t.rand(2,784)\n",
    "net(X)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意，这里并没有将`Module`类命名为`Layer`（层）或者`Model`（模型）之类的名字，这是因为该类是一个**可供自由组建**的部件。它的子类既可以是一个层（如PyTorch提供的Linear类），又可以是一个模型（如这里定义的MLP类），或者是模型的一个部分\n",
    "## `Module`的子类\n",
    "Module类是一个通用的部件，PyTorch还实现了继承自Module的可以方便构建模型的类: 如Sequential、ModuleList和ModuleDict等等\n",
    "### `Sequential`类\n",
    "`Sequential`类可以接收一个子模块的**有序字典（OrderedDict）或者一系列子模块作为参数**来逐一添加`Module`的实例，而模型的前向计算就是将这些实例按添加的顺序逐一计算。\n",
    "\n",
    "下面我们实现一个与`Sequential`类有相同功能的`MySequential`类："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MySequential(\n",
      "  (0): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1460,  0.1617, -0.1478,  0.2251, -0.1415,  0.0834,  0.0942,  0.2068,\n",
       "          0.1312, -0.1836],\n",
       "        [ 0.0809,  0.1665, -0.1793,  0.1542, -0.0970,  0.1232,  0.1412,  0.2647,\n",
       "          0.2444, -0.0876]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MySequential(nn.Module):\n",
    "    from collections import OrderedDict\n",
    "    def __init__(self,*args):\n",
    "        super(MySequential,self).__init__()\n",
    "        if len(args) == 1 and isinstance(args[0],OrderedDict):# 如果传入OrderedDict\n",
    "            for key, module in args[0].items():\n",
    "                self.add_module(key,module) # add_module方法会将module添加进self._modules(一个OrderedDict)\n",
    "        else: #传入的不是OrderedDict而是Module\n",
    "            for idx,module in enumerate(args):\n",
    "                self.add_module(str(idx),module)\n",
    "    def forward(self,input):\n",
    "        for module in self._modules.values():\n",
    "            input=module(input)\n",
    "        return input\n",
    "net = MySequential(\n",
    "        nn.Linear(784, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 10), \n",
    "        )\n",
    "print(net)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`isinstance() `函数来判断一个对象是否是一个已知的类型，`isinstance(object, classinfo)`\n",
    "\n",
    "isinstance() 与 type() 区别：\n",
    "- type() 不会认为子类是一种父类类型，不考虑继承关系。\n",
    "- isinstance() 会认为子类是一种父类类型，考虑继承关系。\n",
    "\n",
    "`enumerate() `函数用于将一个可遍历的数据对象(如列表、元组或字符串)组合为一个索引序列，同时列出**数据和数据下标idx**，一般用在 for 循环当中\n",
    "\n",
    "`Linear`即对输入的数据进行线性变换：`class torch.nn.Linear(in_features, out_features, bias=True)`\n",
    "\n",
    "Applies a linear transformation to the incoming data:$y=x{A}^{T}+b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `ModuleList`类\n",
    "`ModuleList`接收一个子模块的列表作为输入，然后也可以类似List那样进行append和extend操作:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=256, out_features=10, bias=True)\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = nn.ModuleList([nn.Linear(784, 256), nn.ReLU()])\n",
    "net.append(nn.Linear(256, 10)) # # 类似List的append操作\n",
    "print(net[-1])  # 类似List的索引访问\n",
    "print(net)\n",
    "# net(torch.zeros(1, 784)) # 会报NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "既然Sequential和ModuleList都可以进行列表化构造网络，那二者区别是什么呢。\n",
    "\n",
    "ModuleList仅仅是一个储存各种模块的列表，这些模块之间没有联系也没有顺序（所以不用保证相邻层的输入输出维度匹配），而且没有实现forward功能需要自己实现，所以上面执行net(torch.zeros(1, 784))会报NotImplementedError；而Sequential内的模块需要按照顺序排列，要保证相邻层的输入输出大小相匹配，内部forward功能已经实现。\n",
    "\n",
    "`ModuleList`的出现只是让网络定义前向传播时更加灵活，见下面官网的例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModule, self).__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(10)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ModuleList can act as an iterable, or be indexed using ints\n",
    "        for i, l in enumerate(self.linears):\n",
    "            x = self.linears[i // 2](x) + l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另外，ModuleList不同于一般的Python的list，加入到ModuleList里面的所有模块的参数会被自动添加到整个网络中，下面看一个例子对比一下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net1:\n",
      "torch.Size([10, 10])\n",
      "torch.Size([10])\n",
      "net2:\n"
     ]
    }
   ],
   "source": [
    "class Module_ModuleList(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Module_ModuleList, self).__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(10, 10)])\n",
    "\n",
    "class Module_List(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Module_List, self).__init__()\n",
    "        self.linears = [nn.Linear(10, 10)]\n",
    "\n",
    "net1 = Module_ModuleList()\n",
    "net2 = Module_List()\n",
    "\n",
    "print(\"net1:\")\n",
    "for p in net1.parameters():\n",
    "    print(p.size())\n",
    "\n",
    "print(\"net2:\")\n",
    "for p in net2.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `ModuleDict`类\n",
    "`ModuleDict`接收一个子模块的字典作为输入, 然后也可以**类似字典那样**进行添加访问操作:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=784, out_features=256, bias=True)\n",
      "Linear(in_features=256, out_features=10, bias=True)\n",
      "ModuleDict(\n",
      "  (act): ReLU()\n",
      "  (linear): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = nn.ModuleDict({\n",
    "    'linear': nn.Linear(784, 256),\n",
    "    'act': nn.ReLU(),\n",
    "})\n",
    "net['output'] = nn.Linear(256, 10) # 添加\n",
    "print(net['linear']) # 访问\n",
    "print(net.output)\n",
    "print(net)\n",
    "# net(torch.zeros(1, 784)) # 会报NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构造复杂的模型\n",
    "虽然上面介绍的这些类可以使模型构造更加简单，且不需要定义`forward`函数，但**直接继承Module类可以极大地拓展模型构造的灵活性**。下面我们构造一个稍微复杂点的网络FancyMLP。在这个网络中，我们通过`get_constant`函数创建训练中不被迭代的参数，即常数参数。在前向计算中，除了使用创建的常数参数外，我们还使用Tensor的函数和Python的控制流，并多次调用相同的层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FancyMLP(nn.Module):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(FancyMLP,self).__init__(**kwargs)\n",
    "        \n",
    "        self.rand_weight=t.rand((20,20),requires_grad=False) #不可训练参数，自然grad也设置为False\n",
    "        slef.linear=nn.Linear(20,20)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=self.linear(x)# 经过一次线性运算"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "153.438px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
