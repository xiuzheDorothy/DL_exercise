---
layout: reticle
title: 目标检测算法YOLOv3概述
date: 2020-02-04 11:27:22
categories: 目标检测
cover: false
tags:
  - 深度学习
  - 目标检测
---
>[机器之心-从零开始PyTorch项目：YOLO v3目标检测实现](https://www.jiqizhixin.com/articles/2018-04-23-3 "机器之心-从零开始PyTorch项目：YOLO v3目标检测实现")
>[超详细的Pytorch版yolov3代码中文注释详解](https://zhuanlan.zhihu.com/p/49981816 "超详细的Pytorch版yolov3代码中文注释详解")
>英文原文：[How to implement a YOLO (v3) object detector from scratch in PyTorch](https://blog.paperspace.com/how-to-implement-a-yolo-object-detector-in-pytorch/ "How to implement a YOLO (v3) object detector from scratch in PyTorch")
>YOLO的介绍：[YOLO: Real-Time Object Detection](https://pjreddie.com/darknet/yolo/ "YOLO: Real-Time Object Detection")

本文对于机器之心关于YOLO的翻译文章进行转载与进一步润色，因为翻译的有点让人看不懂，机翻痕迹很严重，读来还没有原文清楚。
## 什么是YOLO?
### 网络分析
YOLO 是 You Only Look Once 的缩写。它是一种使用深度卷积神经网络学得的特征来检测对象的目标检测器。YOLO 仅使用卷积层，这就使其成为全卷积神经网络（FCN）它拥有 75 个卷积层，还有跳过连接和上采样层。**它不使用任何形式的池化**，使用步幅为 2 的卷积层对特征图进行下采样。
### 解释输出
在 YOLO 中，预测是通过$$1\times 1$$卷积层完成的，所以其输出是一张特征图(也可以称之为预测图(prediction map))，预测图就是每个可以预测固定数量边界框的单元格。
> 虽然形容特征图中单元的正确术语应该是「神经元」，但本文中为了更为直观，我们将其称为单元格（cell）

$$B\times(5+C)$$：B 代表每个单元可以预测的边界框数量。根据 YOLO 的论文，这些 B 边界框中的每一个都可能专门用于检测某种对象。每个边界框都有 5+C 个属性，分别描述每个边界框的中心坐标、维度、objectness 分数和 C 类置信度。YOLO v3 在每个单元中预测 3 个边界框。

如果对象的中心位于单元格的感受野内，你会希望特征图的每个单元格都可以通过其中一个边界框预测对象。

看下面这张图：
![](https://s2.ax1x.com/2020/02/04/1BAJF1.png)

其中输入图像大小是 416×416，网络的步幅是 32。如之前所述，特征图的维度会是 13×13，所以我们先将这张图片分为13×13个网格。
真值框（ground truth box）是上图中的黄框，其中心为红框。这个红框（更一般性的说法：包含了真值框中心的那个网格）会作为负责预测对象的单元格。

现在，红色单元格是网格中第七行的第七个。我们现在使特征图中第七行第七个单元格作为检测狗的单元。
> 这里还涉及到padding操作，这样会才能让输出特征图单元格的坐标和输入图像的坐标对应

现在，这个单元格可以预测三个边界框（为什么是三个？，因为YOLO3有三个锚点）。哪个将会分配给狗的真值标签？为了理解这一点，我们必须理解锚点的概念。

### 锚点框（Anchor Box）
关于锚点框（简写为锚框）我会单独写一篇文章作分析。简单来说我们对于每一个像素点都会有创建很多个长宽比不同的方框（就是锚框）。**在训练集中**我们标注其类别和相对于真值框的偏移量。**在目标检测时**，我们首先生成多个锚框，然后为每个锚框预测类别以及偏移量，接着根据预测的偏移量调整锚框位置从而得到预测边界框，最后筛选需要输出的预测边界框。

总的来说，我们要求出IoU(最大交并比)。
### 预测
![1BKWZT.png](https://s2.ax1x.com/2020/02/04/1BKWZT.png)
bx, by, bw, bh是预测的x,y中心坐标，宽度和高度。tx, ty, tw, th是网络的输出。cx和cy是网格的左上角坐标。pw和ph是边界框的锚定尺寸。
### 中心坐标
注意：我们使用 sigmoid 函数进行中心坐标预测。这使得输出值在 0 和 1 之间。原因如下：

正常情况下，YOLO 不会预测边界框中心的确切坐标。它预测两个偏移量：

- 与预测目标的网格单元左上角相关的偏移；
- 使用特征图单元的维度（1）进行归一化的偏移。

以我们的那张小狗图像为例。如果中心的预测是 (0.4, 0.7)，则中心在 13 x 13 特征图上的坐标是 (6.4, 6.7)（红色单元的左上角坐标是 (6,6)）。

但是，如果预测到的 x,y 坐标大于 1，比如 (1.2, 0.7)。那么中心坐标是 (7.2, 6.7)。注意该中心在红色单元右侧的单元中，或第 7 行的第 8 个单元。**这打破了 YOLO 背后的理论**，因为如果我们假设红色框负责预测目标狗，那么狗的中心必须在红色单元中，不应该在它旁边的网格单元中。

因此，为了解决这个问题，我们对输出执行 sigmoid 函数，将输出压缩到区间 0 到 1 之间，有效确保中心处于执行预测的网格单元中。
### 边界框的维度
对网络输出进行对数空间变换，然后乘以锚点，得到的结果作为我们的预测框的维度
![1BMYY4.png](https://s2.ax1x.com/2020/02/04/1BMYY4.png)
*图源：http://christopher5106.github.io/*
### Objectness 分数
Object 分数表示目标在边界框内的概率。红色网格和相邻网格的 Object 分数应该接近 1，而角落处的网格的 Object 分数可能接近 0。

objectness 分数的计算也使用 sigmoid 函数，因此它可以被理解为概率。

### 在不同尺度上的预测
YOLO v3 在 3 个不同尺度上进行预测。检测层用于在三个不同大小的特征图上执行预测，特征图步幅分别是 32、16、8。这意味着，当输入图像大小是 416 x 416 时，我们在尺度 13 x 13、26 x 26 和 52 x 52 上执行检测。

该网络在第一个检测层之前对输入图像执行下采样；检测层使用步幅为 32 的层的特征图执行检测。随后在步长为 2 的上采样后，并与前一个层的特征图（特征图大小相同）拼接。c另一个检测在步幅为 16 的层中执行。重复同样的上采样步骤，最后一个检测在步幅为 8 的层中执行。

在每个尺度上，每个单元使用 3 个锚点预测 3 个边界框，锚点的总数为 9（不同尺度的锚点不同）。
![1BMvn0.png](https://s2.ax1x.com/2020/02/04/1BMvn0.png)

### 输出处理

对于大小为 416 x 416 的图像，YOLO 预测 ((52 x 52) + (26 x 26) + 13 x 13)) x 3 = 10647 个边界框。但是，我们的示例中只有一个对象——一只狗。那么我们怎么才能将检测次数从 10647 减少到 1 呢？

- 目标置信度阈值：首先，我们根据它们的 objectness 分数过滤边界框。通常，分数低于阈值的边界框会被忽略。

- 非极大值抑制：非极大值抑制（NMS）可解决对同一个图像的多次检测的问题。例如，红色网格单元的 3 个边界框可以检测一个框，或者临近网格可检测相同对象。

### 具体实现
具体实现过几天我会以代码的形式呈现，主要参考[YOLO_v3_tutorial_from_scratch](https://github.com/ayooshkathuria/YOLO_v3_tutorial_from_scratch "YOLO_v3_tutorial_from_scratch")和[pytorch-yolo-v3](https://github.com/ayooshkathuria/pytorch-yolo-v3 "pytorch-yolo-v3")，please waiting for！😀